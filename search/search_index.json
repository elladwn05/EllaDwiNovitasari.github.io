{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Biodata Nama : Ella Dwi Novitasari NIM : 180411100126 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab,S.SI.,M.kom Jurusan : Teknik Informatika Alamat : Perumahan Graha Indah N-12A, Lamongan","title":"Biodata"},{"location":"#biodata","text":"Nama : Ella Dwi Novitasari NIM : 180411100126 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab,S.SI.,M.kom Jurusan : Teknik Informatika Alamat : Perumahan Graha Indah N-12A, Lamongan","title":"Biodata"},{"location":"Statistik Deskriptif/","text":"Pengertian Statistik Deskriptif adalah metode \u2013 metode yang juga berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga dapat memberikan informasi yang berguna. Statistik Deskriptif juga merupakan metode yang sangat sederhana. Metode ini hanya mendeskripsikan kondisi dari data yang sudah anda miliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang disajikan dalam uraian \u2013 uraian singkat dan juga terbatas. \u200b Statistik deskriptif hanya terbatas dalam menyajikan data dalam bentuk tabel, diagram, grafik, dan besaran lain, sedangkan statistik inferensial selain mencakup statistik deskriptif juga dapat digunakan untuk melakukan estimasi dan penarikan kesimpulan terhadap populasi dari sampelnya. Untuk sampai pada penarikan kesimpulan statistik inferensia melalui tahap uji hipotesis dan uji statistik. Tipe Data Statistik Deskriptif Mean ( Rata-rata ) \u200b Mean merupakan teknik penjelasan kelompok yang didasarkan atas nilai rata-rata dari kelompok tersebut. Mean ini didapat dengan menjumlahkan data seluruh individu dalam kelompok itu, kemudian dibagi dengan jumlah individu yang ada pada kelompok tersebut. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Diketahui : x bar = x rata - rata = nilai rata - rata sampel x = data ke n n = banyaknya data Median ( Nilai Tengah ) \u200b Median merupakan salah satu teknik penjelasan kelompok yang didasarkan atas nilai tengah dari kelompok data yang telah disusun urutannya dari yang terkecil sampai yang terbesar, atau sebaliknya dari yang terbesar sampai yang terkecil. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ Diketahui : Me = media dari kelompok data n = banyak data Modus \u200b Modus merupakan teknik penjelasan kelompok yang didasarkan atas nilai yang sedang populer (yang sedang menjadi mode) atau nilai yang sering muncul dalam kelompok tersebut. Terdapat dua macam modus untuk sebuah data yaitu modus untuk data tunggal dan modus untuk data golongan. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Diketahui : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemen sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen selanjutnya p = panjang interval nilai b1 dan b2 selalu mutlak(bernilai positif) Varian dan Standar Deviasi \u200b Standar deviasi dan varian salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varian merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. \u200b Standar deviasi dan varians simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Diketahui : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari anggota data Skewness \u200b Skewness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan miring atau tidaknya bentuk kurva suatu distribusi frekuensi. Skewness adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Diketahui : Xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile \u200b Quartile adalah nilai-nilai yang membagi data dalam 4 bagian yang sama, atau setiap bagian dari kuartil sebesar 25%. Quartile itu ada 3, yaitu quartile pertama, kedua, dan ketiga. Adapun cara mencarinya hampir sama dengan cara mencari median, perbedaanya pada letak quartile. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Diketahui : Q = nilai dari quartile n = banyak dari himpunan data Penerapan Statistik Deskriptif import pandas as pd from scipy import stats df = pd.read_csv('Tugas 1_Penambangan Data.csv',sep=';') data = {\"stats\": ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes','Quantile 1','Quantile 2', 'Quantile 3','Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(),2), round(df[i].var(),2), round(df[i].skew(),2), df[i].quantile(0.25),df[i].quantile(0.5),df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]] tes = pd.DataFrame(data) tes.style.hide_index() Hasil run : stats Tinggi Badan Berat Badan Usia Lingkar Badan Min 140 40 20 70 Max 190 70 50 100 Mean 164.882 54.72 34.832 85.228 Standard Deviasi 15.18 8.96 9.3 8.8 Variasi 230.35 80.27 86.4 77.42 Skewnes -0 0.1 0.08 -0.07 Quantile 1 151 47 27 78 Quantile 2 165 54 34 85 Quantile 3 179 63 43.25 93 Median 165 54 34 85 Modus 142 50 28 93 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik Deskriptif"},{"location":"Statistik Deskriptif/#pengertian","text":"Statistik Deskriptif adalah metode \u2013 metode yang juga berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga dapat memberikan informasi yang berguna. Statistik Deskriptif juga merupakan metode yang sangat sederhana. Metode ini hanya mendeskripsikan kondisi dari data yang sudah anda miliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang disajikan dalam uraian \u2013 uraian singkat dan juga terbatas. \u200b Statistik deskriptif hanya terbatas dalam menyajikan data dalam bentuk tabel, diagram, grafik, dan besaran lain, sedangkan statistik inferensial selain mencakup statistik deskriptif juga dapat digunakan untuk melakukan estimasi dan penarikan kesimpulan terhadap populasi dari sampelnya. Untuk sampai pada penarikan kesimpulan statistik inferensia melalui tahap uji hipotesis dan uji statistik.","title":"Pengertian"},{"location":"Statistik Deskriptif/#tipe-data-statistik-deskriptif","text":"","title":"Tipe Data Statistik Deskriptif"},{"location":"Statistik Deskriptif/#mean-rata-rata","text":"\u200b Mean merupakan teknik penjelasan kelompok yang didasarkan atas nilai rata-rata dari kelompok tersebut. Mean ini didapat dengan menjumlahkan data seluruh individu dalam kelompok itu, kemudian dibagi dengan jumlah individu yang ada pada kelompok tersebut. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Diketahui : x bar = x rata - rata = nilai rata - rata sampel x = data ke n n = banyaknya data","title":"Mean ( Rata-rata )"},{"location":"Statistik Deskriptif/#median-nilai-tengah","text":"\u200b Median merupakan salah satu teknik penjelasan kelompok yang didasarkan atas nilai tengah dari kelompok data yang telah disusun urutannya dari yang terkecil sampai yang terbesar, atau sebaliknya dari yang terbesar sampai yang terkecil. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ Diketahui : Me = media dari kelompok data n = banyak data","title":"Median ( Nilai Tengah )"},{"location":"Statistik Deskriptif/#modus","text":"\u200b Modus merupakan teknik penjelasan kelompok yang didasarkan atas nilai yang sedang populer (yang sedang menjadi mode) atau nilai yang sering muncul dalam kelompok tersebut. Terdapat dua macam modus untuk sebuah data yaitu modus untuk data tunggal dan modus untuk data golongan. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Diketahui : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemen sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen selanjutnya p = panjang interval nilai b1 dan b2 selalu mutlak(bernilai positif)","title":"Modus"},{"location":"Statistik Deskriptif/#varian-dan-standar-deviasi","text":"\u200b Standar deviasi dan varian salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varian merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. \u200b Standar deviasi dan varians simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Diketahui : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari anggota data","title":"Varian dan Standar Deviasi"},{"location":"Statistik Deskriptif/#skewness","text":"\u200b Skewness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan miring atau tidaknya bentuk kurva suatu distribusi frekuensi. Skewness adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Diketahui : Xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi","title":"Skewness"},{"location":"Statistik Deskriptif/#quartile","text":"\u200b Quartile adalah nilai-nilai yang membagi data dalam 4 bagian yang sama, atau setiap bagian dari kuartil sebesar 25%. Quartile itu ada 3, yaitu quartile pertama, kedua, dan ketiga. Adapun cara mencarinya hampir sama dengan cara mencari median, perbedaanya pada letak quartile. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Diketahui : Q = nilai dari quartile n = banyak dari himpunan data","title":"Quartile"},{"location":"Statistik Deskriptif/#penerapan-statistik-deskriptif","text":"import pandas as pd from scipy import stats df = pd.read_csv('Tugas 1_Penambangan Data.csv',sep=';') data = {\"stats\": ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes','Quantile 1','Quantile 2', 'Quantile 3','Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(),2), round(df[i].var(),2), round(df[i].skew(),2), df[i].quantile(0.25),df[i].quantile(0.5),df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]] tes = pd.DataFrame(data) tes.style.hide_index() Hasil run : stats Tinggi Badan Berat Badan Usia Lingkar Badan Min 140 40 20 70 Max 190 70 50 100 Mean 164.882 54.72 34.832 85.228 Standard Deviasi 15.18 8.96 9.3 8.8 Variasi 230.35 80.27 86.4 77.42 Skewnes -0 0.1 0.08 -0.07 Quantile 1 151 47 27 78 Quantile 2 165 54 34 85 Quantile 3 179 63 43.25 93 Median 165 54 34 85 Modus 142 50 28 93 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Penerapan Statistik Deskriptif"},{"location":"Tugas 2 - Mengukur Jarak/","text":"Mengukur Jarak \u200b Pengukuran jarak memegang peran yang sangat penting dalam menentukan kemiripan atau keteraturan di antara data dan item. Hal ini dilakukan untuk mengetahui, dengan cara seperti apa data dikatakan saling terkait, mirip, tidak mirip, dan metode pengukuran jarak seperti apa yang diperlukan untuk membandingkannya. Pada proses clustering, tahapan menentukan atau mendeskripsikan nilai kuantitatif dari tingkat kemiripan atau ketidakmiripan data (proximity measure) memiliki peranan sangat penting, sehingga perlu dilakukannya perbandingan beberapa metode yang sering digunakan, yaitu jarak euclidean, manhattan, dan minkowski. Mengukur jarak menggunakan Eucliean Distance \u200b Euclidean distance merupakan salah satu metode perhitungan jarak yang digunakan untuk mengukur jarak dari 2(dua) buah titik dalam Euclidean space (meliputi bidang euclidean dua dimensi, tiga dimensi, atau bahkan lebih). Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Mengukur jarak menggunakan Manhattan Distance \u200b Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Manhattan distance digunakan untuk menghitung perbedaan absolut (mutlak) antara koordinat sepasang objek. Rumus yang digunakan adalah sebagai berikut: $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ dimana m merupakan bilangan riel positif xi dan yi merupakan dua vektor dalam ruang dimensi n implementasi ukuran jarak minkowski pada model clustering data atribut dilakukan normalisasi untuk mengukur dominasi dari atribut yang memiliki skala data besar,. Mengukur jarak menggunakan Minkowski Distance \u200b Minkowski distance merupakan sebuah metrik dalam ruang vektor di mana suatu norma didefinisikan (normed vector space) sekaligus dianggap sebagai generalisasi dari Euclidean distance dan Manhattan distance. Dalam pengukuran jarak objek menggunakan minkowski distance biasanya digunakan nilai p adalah 1 atau 2. Berikut rumus yang digunakan menghitung jarak dalam metode ini. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); Langkah mengukur jarak : #Tugas 2_Penambangan Data.csv import pandas as pd from scipy import stats df = pd.read_csv('Tugas 2_Penambangan Data.csv',nrows=4,sep=';') df import scipy.spatial.distance as minko import itertools def minkowski (x,y,data): return sum(x)+sum(y) dfvalues = df.values.tolist() data = [ [x[0],x[1],minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 1) ,minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 2)] for x in itertools.combinations(range(4),2) ] columns = ['x','y', 'Minkowski (m-1)', 'Minkowski (m-2)'] pd.DataFrame(data, columns=columns) numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"x\"]+['y']+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [0]+[2]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [0]+[3]+[0]+[\"{:.2f}\".format(chordDist(0,3,numerical))]+[0]+[0]+[0], [1]+[2]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [1]+[3]+[0]+[\"{:.2f}\".format(chordDist(1,3,numerical))]+[0]+[0]+[0], [2]+[3]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html')))","title":"Mengukur Jarak"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak","text":"\u200b Pengukuran jarak memegang peran yang sangat penting dalam menentukan kemiripan atau keteraturan di antara data dan item. Hal ini dilakukan untuk mengetahui, dengan cara seperti apa data dikatakan saling terkait, mirip, tidak mirip, dan metode pengukuran jarak seperti apa yang diperlukan untuk membandingkannya. Pada proses clustering, tahapan menentukan atau mendeskripsikan nilai kuantitatif dari tingkat kemiripan atau ketidakmiripan data (proximity measure) memiliki peranan sangat penting, sehingga perlu dilakukannya perbandingan beberapa metode yang sering digunakan, yaitu jarak euclidean, manhattan, dan minkowski.","title":"Mengukur Jarak"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak-menggunakan-eucliean-distance","text":"\u200b Euclidean distance merupakan salah satu metode perhitungan jarak yang digunakan untuk mengukur jarak dari 2(dua) buah titik dalam Euclidean space (meliputi bidang euclidean dua dimensi, tiga dimensi, atau bahkan lebih). Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Mengukur jarak menggunakan Eucliean Distance"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak-menggunakan-manhattan-distance","text":"\u200b Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Manhattan distance digunakan untuk menghitung perbedaan absolut (mutlak) antara koordinat sepasang objek. Rumus yang digunakan adalah sebagai berikut: $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ dimana m merupakan bilangan riel positif xi dan yi merupakan dua vektor dalam ruang dimensi n implementasi ukuran jarak minkowski pada model clustering data atribut dilakukan normalisasi untuk mengukur dominasi dari atribut yang memiliki skala data besar,.","title":"Mengukur jarak menggunakan Manhattan Distance"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak-menggunakan-minkowski-distance","text":"\u200b Minkowski distance merupakan sebuah metrik dalam ruang vektor di mana suatu norma didefinisikan (normed vector space) sekaligus dianggap sebagai generalisasi dari Euclidean distance dan Manhattan distance. Dalam pengukuran jarak objek menggunakan minkowski distance biasanya digunakan nilai p adalah 1 atau 2. Berikut rumus yang digunakan menghitung jarak dalam metode ini. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur jarak menggunakan Minkowski Distance"},{"location":"Tugas 2 - Mengukur Jarak/#langkah-mengukur-jarak","text":"#Tugas 2_Penambangan Data.csv import pandas as pd from scipy import stats df = pd.read_csv('Tugas 2_Penambangan Data.csv',nrows=4,sep=';') df import scipy.spatial.distance as minko import itertools def minkowski (x,y,data): return sum(x)+sum(y) dfvalues = df.values.tolist() data = [ [x[0],x[1],minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 1) ,minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 2)] for x in itertools.combinations(range(4),2) ] columns = ['x','y', 'Minkowski (m-1)', 'Minkowski (m-2)'] pd.DataFrame(data, columns=columns) numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"x\"]+['y']+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [0]+[2]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [0]+[3]+[0]+[\"{:.2f}\".format(chordDist(0,3,numerical))]+[0]+[0]+[0], [1]+[2]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [1]+[3]+[0]+[\"{:.2f}\".format(chordDist(1,3,numerical))]+[0]+[0]+[0], [2]+[3]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html')))","title":"Langkah mengukur jarak :"},{"location":"Tugas 3 - Missing Value with KNN/","text":"Missing Value with KNN Missing Data adalah suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. \u200b Salah satu metode imputasi adalah metode K Nearest Neighbor(KNN). Penelitian ini dilakukan untuk memprediksi nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K adalah jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15. Imputasi dengan Metode KNN \u200b Salah satu metode yang sering digunakan untuk masalah imputasi missing data adalah KNN. Metode ini merupakan metode yang sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc Kelemahan dan Keuntungan KNN \u200b Keuntungan KNN adalah karena metode ini dapat digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. Selain itu Algoritma KNN akan mencari data yang paling mirip pada seluruh data pada dataset. Hal ini mengakibatkan kompleksitas waktu yang dibutuhkan algoritma ini menjadi cukup tinggi. Kelemahan yang ketiga adalah Pemilihan nilai k yang tidak tepat dapat menurukan kinerja klasifikasi (Acuna, 2004). # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = {'First Score':[100, 90, np.nan, 95], 'Second Score': [30, 45, 56, np.nan], 'Third Score':[np.nan, 40, 80, 98]} # creating a dataframe from dictionary df = pd.DataFrame(dict) # filling missing value using fillna() df.fillna(0)","title":"Missing Value with KNN"},{"location":"Tugas 3 - Missing Value with KNN/#missing-value-with-knn","text":"Missing Data adalah suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. \u200b Salah satu metode imputasi adalah metode K Nearest Neighbor(KNN). Penelitian ini dilakukan untuk memprediksi nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K adalah jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15.","title":"Missing Value with KNN"},{"location":"Tugas 3 - Missing Value with KNN/#imputasi-dengan-metode-knn","text":"\u200b Salah satu metode yang sering digunakan untuk masalah imputasi missing data adalah KNN. Metode ini merupakan metode yang sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc","title":"Imputasi dengan Metode KNN"},{"location":"Tugas 3 - Missing Value with KNN/#kelemahan-dan-keuntungan-knn","text":"\u200b Keuntungan KNN adalah karena metode ini dapat digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. Selain itu Algoritma KNN akan mencari data yang paling mirip pada seluruh data pada dataset. Hal ini mengakibatkan kompleksitas waktu yang dibutuhkan algoritma ini menjadi cukup tinggi. Kelemahan yang ketiga adalah Pemilihan nilai k yang tidak tepat dapat menurukan kinerja klasifikasi (Acuna, 2004). # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = {'First Score':[100, 90, np.nan, 95], 'Second Score': [30, 45, 56, np.nan], 'Third Score':[np.nan, 40, 80, 98]} # creating a dataframe from dictionary df = pd.DataFrame(dict) # filling missing value using fillna() df.fillna(0)","title":"Kelemahan dan Keuntungan KNN"},{"location":"Tugas 4 - Decision Tree/","text":"Decision Tree Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. \u200b Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. \u200b Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. \u200b Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Entropy \u200b Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row Gain \u200b Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur Gini Index \u200b Dalam penerapan GINI index untuk data berskala continuous , terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode brute-force dan metode midpoints . # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata(): balance_data = pd.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-'+ 'databases/balance-scale/balance-scale.data', sep= ',', header = None) # Printing the dataswet shape print (\"Dataset Length: \", len(balance_data)) print (\"Dataset Shape: \", balance_data.shape) # Printing the dataset obseravtions print (\"Dataset: \",balance_data.head()) return balance_data # Function to split the dataset def splitdataset(balance_data): # Seperating the target variable X = balance_data.values[:, 1:5] Y = balance_data.values[:, 0] # Spliting the dataset into train and test X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100) return X, Y, X_train, X_test, y_train, y_test # Function to perform training with giniIndex. def train_using_gini(X_train, X_test, y_train): # Creating the classifier object clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5) # Performing training clf_gini.fit(X_train, y_train) return clf_gini # Function to perform training with entropy. def tarin_using_entropy(X_train, X_test, y_train): # Decision tree with entropy clf_entropy = DecisionTreeClassifier( criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5) # Performing training clf_entropy.fit(X_train, y_train) return clf_entropy # Function to make predictions def prediction(X_test, clf_object): # Predicton on test with giniIndex y_pred = clf_object.predict(X_test) print(\"Predicted values:\") print(y_pred) return y_pred # Function to calculate accuracy def cal_accuracy(y_test, y_pred): print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred)) print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) print(\"Report : \", classification_report(y_test, y_pred)) # Driver code def main(): # Building Phase data = importdata() X, Y, X_train, X_test, y_train, y_test = splitdataset(data) clf_gini = train_using_gini(X_train, X_test, y_train) clf_entropy = tarin_using_entropy(X_train, X_test, y_train) # Operational Phase print(\"Results Using Gini Index:\") # Prediction using gini y_pred_gini = prediction(X_test, clf_gini) cal_accuracy(y_test, y_pred_gini) print(\"Results Using Entropy:\") # Prediction using entropy y_pred_entropy = prediction(X_test, clf_entropy) cal_accuracy(y_test, y_pred_entropy) # Calling main function if __name__==\"__main__\": main() Dataset Length: 625 Dataset Shape: (625, 5) Dataset: 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index: Predicted values: ['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 67 18] [ 0 19 71]] Accuracy : 73.40425531914893 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy: Predicted values: ['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 63 22] [ 0 20 70]] Accuracy : 70.74468085106383 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Decision Tree"},{"location":"Tugas 4 - Decision Tree/#decision-tree","text":"Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. \u200b Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. \u200b Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. \u200b Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain.","title":"Decision Tree"},{"location":"Tugas 4 - Decision Tree/#entropy","text":"\u200b Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row","title":"Entropy"},{"location":"Tugas 4 - Decision Tree/#gain","text":"\u200b Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur","title":"Gain"},{"location":"Tugas 4 - Decision Tree/#gini-index","text":"\u200b Dalam penerapan GINI index untuk data berskala continuous , terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode brute-force dan metode midpoints . # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata(): balance_data = pd.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-'+ 'databases/balance-scale/balance-scale.data', sep= ',', header = None) # Printing the dataswet shape print (\"Dataset Length: \", len(balance_data)) print (\"Dataset Shape: \", balance_data.shape) # Printing the dataset obseravtions print (\"Dataset: \",balance_data.head()) return balance_data # Function to split the dataset def splitdataset(balance_data): # Seperating the target variable X = balance_data.values[:, 1:5] Y = balance_data.values[:, 0] # Spliting the dataset into train and test X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100) return X, Y, X_train, X_test, y_train, y_test # Function to perform training with giniIndex. def train_using_gini(X_train, X_test, y_train): # Creating the classifier object clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5) # Performing training clf_gini.fit(X_train, y_train) return clf_gini # Function to perform training with entropy. def tarin_using_entropy(X_train, X_test, y_train): # Decision tree with entropy clf_entropy = DecisionTreeClassifier( criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5) # Performing training clf_entropy.fit(X_train, y_train) return clf_entropy # Function to make predictions def prediction(X_test, clf_object): # Predicton on test with giniIndex y_pred = clf_object.predict(X_test) print(\"Predicted values:\") print(y_pred) return y_pred # Function to calculate accuracy def cal_accuracy(y_test, y_pred): print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred)) print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) print(\"Report : \", classification_report(y_test, y_pred)) # Driver code def main(): # Building Phase data = importdata() X, Y, X_train, X_test, y_train, y_test = splitdataset(data) clf_gini = train_using_gini(X_train, X_test, y_train) clf_entropy = tarin_using_entropy(X_train, X_test, y_train) # Operational Phase print(\"Results Using Gini Index:\") # Prediction using gini y_pred_gini = prediction(X_test, clf_gini) cal_accuracy(y_test, y_pred_gini) print(\"Results Using Entropy:\") # Prediction using entropy y_pred_entropy = prediction(X_test, clf_entropy) cal_accuracy(y_test, y_pred_entropy) # Calling main function if __name__==\"__main__\": main() Dataset Length: 625 Dataset Shape: (625, 5) Dataset: 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index: Predicted values: ['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 67 18] [ 0 19 71]] Accuracy : 73.40425531914893 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy: Predicted values: ['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 63 22] [ 0 20 70]] Accuracy : 70.74468085106383 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Gini Index"},{"location":"Tugas 5 - Clustering/","text":"Clustering Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM). Clustering dengan pendekatan partisi 1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah. Rumus K-Means Metode K-Modes \u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster. Metode K-Prototype \u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu. Algoritma K-Prototype \u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek. Rumus K-Prototype 2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya. Clustering dengan Pendekatan Hirarki \u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa. Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM) \u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering"},{"location":"Tugas 5 - Clustering/#clustering","text":"Clustering adalah metode penganalisaan data yang sering dimasukkan sebagai salah satu metode Data Mining yang tujuannya adalah untuk mengelompokkan data dengan karakteristik yang sama ke suatu wilayah yang sama dan data dengan karakteristik yang berbeda ke wilayah yang lain. \u200b Ada beberapa pendekatan yang digunakan dalam mengembangkan metode clustering, dua pendekatan utama adalah clustering dengan pendekatan partisi dan clustering dengan pendekatan hirarki. Clustering dengan pendekatan partisi atau sering disebut dengan partition-based clustering mengelompokkan data dengan memilah-milah data yang dianalisa ke dalam cluster-cluster yang ada. Clustering dengan pendekatan hirarki atau sering disebut dengan hierarchical clustering mengelompokkan data dengan membuat suatu hirarki berupa dendogram dimana data yang mirip akan ditempatkan pada hirarki yang berdekatan dan yang tidak pada hirarki yang berjauhan. Di samping kedua pendekatan tersebut, ada juga clustering dengan pendekatan automatic mapping (Self-Organising Map/SOM).","title":"Clustering"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-partisi","text":"1. K-Means \u200b Salah satu metode yang banyak digunakan dalam melakukan clustering dengan partisi ini adalah metode k-means. Secara umum metode k-means ini melakukan proses pengelompokan dengan prosedur sebagai berikut: \u00b7 Tentukan jumlah cluster \u00b7 Alokasikan data secara random ke cluster yang ada \u00b7 Hitung rata-rata setiap cluster dari data yang tergabung di dalamnya \u00b7 Alokasikan kembali semua data ke cluster terdekat \u00b7 Ulang proses nomor 3, sampai tidak ada perubahan atau perubahan yang terjadi masih sudah di bawah treshold \u200b Prosedur dasar ini bisa berubah mengikuti pendekatan pengalokasian data yang diterapkan, apakah crisp atau fuzzy . Setelah meneliti clustering dari sudut yang lain, saya menemukan bahwa k-means clustering mempunyai beberapa kelemahan. Fungsi dari algoritma ini adalah mengelompokkan data kedalam beberapa cluster. karakteristik dari algoritma ini adalah : . Memiliki n buah data. . Input berupa jumlah data dan jumlah cluster (kelompok). . Pada setiap cluster/kelompok memiliki sebuah centroid yang mempresentasikan cluster tersebut. Algoritma K-Means \u200b Secara sederhana algoritma K-Means dimulai dari tahap berikut : . Pilih K buah titik centroid. . Menghitung jarak data dengan centroid. . Update nilai titik centroid. . Ulangi langkah 2 dan 3 sampai nilai dari titik centroid tidak lagi berubah.","title":"Clustering dengan pendekatan partisi"},{"location":"Tugas 5 - Clustering/#rumus-k-means","text":"","title":"Rumus K-Means"},{"location":"Tugas 5 - Clustering/#metode-k-modes","text":"\u200b K-Modes merupakan pengembangan dari algoritma clustering K-means untuk menangani data kategorik di mana means diganti oleh modes. K-Modes menggunakan simple matching meassure dalam penentuan similarity dari suatu klaster.","title":"Metode K-Modes"},{"location":"Tugas 5 - Clustering/#metode-k-prototype","text":"\u200b Tujuan dari simulasi ini adalah mencoba menerapkan algoritma K-Prototype pada data campuran numerik dan kategorikal. Ada tahap preparation diperlakukan terhadap data point numerik normalisasi terlebih dahulu.","title":"Metode K-Prototype"},{"location":"Tugas 5 - Clustering/#algoritma-k-prototype","text":"\u200b Sebelum masuk proses algoritma K-Prototypes tentukan jumlah k yang akan dibentuk batasannya minimal 2 dan maksimal \u221an atau n/2 dimana n adalah jumlah data point atau obyek . Tahap 1 : Tentukan K dengan inisial kluster z1, z2, ..., zk secara acak dari n buah titik {x1, x2, ..., xn} . Tahap 2 : Hitung jarak seluruh data point pada data set terhadap inisial kluster awal, alokasikan data point ke dalam cluster yang memiliki jarak prototype terdekat dengan object yang diukur. . Tahap 3 : Hitung titik pusat cluster yang baru setelah semua objek dialokasikan. Lalu realokasikan semua datapoint pada dataset terhadap prototype yang baru. . Tahap 4 : jika titik pusat cluster tidak berubah atau sudah konvergen maka proses algoritma berhenti tetapi jika titik pusat masih berubah-ubah secara signifikan maka proses kembali ke tahap 2 dan 3 hingga iterasi maksimum tercapai atau sudah tidak ada perpindahan objek.","title":"Algoritma K-Prototype"},{"location":"Tugas 5 - Clustering/#rumus-k-prototype","text":"2. Mixture Modelling (Mixture Modeling) \u200b Mixture modelling merupakan metode pengelompokan data yang mirip dengan k-means dengan kelebihan penggunaan distribusi statistik dalam mendefinisikan setiap cluster yang ditemukan. Dibandingkan dengan k-means yang hanya menggunakan cluster center, penggunaan distribusi statistik ini mengijinkan kita untuk: \u00b7 Memodel data yang kita miliki dengan setting karakteristik yang berbeda-beda \u00b7 Jumlah cluster yang sesuai dengan keadaan data bisa ditemukan seiring dengan proses pemodelan karakteristik dari masing-masing cluster \u00b7 Hasil pemodelan clustering yang dilaksanakan bisa diuji tingkat keakuratannya \u200b Distribusi statistik yang digunakan bisa bermacam-macam mulai dari yang digunakan untuk data categorical sampai yang continuous, termasuk di antaranya distribusi binomial, multinomial, normal dan lain-lain. Beberapa distribusi yang bersifat tidak normal seperti distribusi Poisson, von-Mises, Gamma dan Student t, juga diterapkan untuk bisa mengakomodasi berbagai keadaan data yang ada di lapangan. Beberapa pendekatan multivariate juga banyak diterapkan untuk memperhitungkan tingkat keterkaitan antara variabel data yang satu dengan yang lainnya.","title":"Rumus K-Prototype"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-hirarki","text":"\u200b Clustering dengan pendekatan hirarki mengelompokkan data yang mirip dalam hirarki yang sama dan yang tidak mirip di hirarki yang agak jauh. Ada dua metode yang sering diterapkan yaitu agglomerative hieararchical clustering dan divisive hierarchical clustering . Agglomerative melakukan proses clustering dari N cluster menjadi satu kesatuan cluster, dimana N adalah jumlah data, sedangkan divisive melakukan proses clustering yang sebaliknya yaitu dari satu cluster menjadi N cluster. \u200b Beberapa metode hierarchical clustering yang sering digunakan dibedakan menurut cara mereka untuk menghitung tingkat kemiripan. Ada yang menggunakan Single Linkage , Complete Linkage , Average Linkage , Average Group Linkage dan lain-lainnya. Seperti juga halnya dengan partition-based clustering , kita juga bisa memilih jenis jarak yang digunakan untuk menghitung tingkat kemiripan antar data. \u200b Salah satu cara untuk mempermudah pengembangan dendogram untuk hierarchical clustering ini adalah dengan membuat similarity matrix yang memuat tingkat kemiripan antar data yang dikelompokkan. Tingkat kemiripan bisa dihitung dengan berbagai macam cara seperti dengan Euclidean Distance Space. Berangkat dari similarity matrix ini, kita bisa memilih lingkage jenis mana yang akan digunakan untuk mengelompokkan data yang dianalisa.","title":"Clustering dengan Pendekatan Hirarki"},{"location":"Tugas 5 - Clustering/#clustering-dengan-pendekatan-automatic-mapping-self-organising-mapsom","text":"\u200b Self-Organising Map merupakan suatu tipe Artificial Neural Networks yang di-training secara unsupervised. SOM menghasilkan map yang terdiri dari output dalam dimensi yang rendah (2 atau 3 dimensi). Map ini berusaha mencari property dari input data. Komposisi input dan output dalam SOM mirip dengan komposisi dari proses feature scaling (multidimensional scaling). \u200b Walaupun proses learning yang dilakukan mirip dengan Artificial Neural Networks, tetapi proses untuk meng-assign input data ke map, lebih mirip dengan K-Means dan kNN Algorithm. Adapun prosedur yang ditempuh dalam melakukan clustering dengan SOM adalah sebagai berikut: \u00b7 Tentukan weight dari input data secara random \u00b7 Pilih salah satu input data \u00b7 Hitung tingkat kesamaan (dengan Eucledian) antara input data dan weight dari input data tersebut dan pilih input data yang memiliki kesamaan dengan weight yang ada (data ini disebut dengan Best Matching Unit (BMU)) \u00b7 Perbaharui weight dari input data dengan mendekatkan weight tersebut ke BMU dengan rumus: Wv(t+1) = Wv(t) + Theta(v, t) x Alpha(t) x (D(t) \u2013 Wv(t)) Dimana: o Wv(t) : Weight pada saat ke-t o Theta (v, t) : Fungsi neighbourhood yang tergantung pada Lattice distance antara BMU dengan neuron v. Umumnya bernilai 1 untuk neuron yang cukup dekat dengan BMU, dan 0 untuk yang sebaliknya. Penggunaan fungsi Gaussian juga memungkinkan. o Alpha (t) : Learning Coefficient yang berkurang secara monotonic o D(t) : Input data \u00b7 Tambah nilai t, sampai t < Lambda , dimana Lambda adalah jumlah iterasi MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Clustering Dengan Pendekatan Automatic Mapping (Self-Organising Map/SOM)"},{"location":"Tugas 6 - Fuzzy C-Means/","text":"Fuzzy C-Means \u200b Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. \u200b Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. \u200b Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT). Algoritma fuzzy c-means from fcmeans import FCM from sklearn.datasets import make_blobs from matplotlib import pyplot as plt from seaborn import scatterplot as scatter # create artifitial dataset n_samples = 50000 n_bins = 3 # use 3 bins for calibration_curve as we have 3 clusters here centers = [(-5, -5), (0, 0), (5, 5)] X,_ = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0, centers=centers, shuffle=False, random_state=42) # fit the fuzzy-c-means fcm = FCM(n_clusters=3) fcm.fit(X) # outputs fcm_centers = fcm.centers fcm_labels = fcm.u.argmax(axis=1) # plot result %matplotlib inline f, axes = plt.subplots(1, 2, figsize=(11,5)) scatter(X[:,0], X[:,1], ax=axes[0]) scatter(X[:,0], X[:,1], ax=axes[1], hue=fcm_labels) scatter(fcm_centers[:,0], fcm_centers[:,1], ax=axes[1],marker=\"s\",s=200) plt.show() MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Fuzzy C-Means"},{"location":"Tugas 6 - Fuzzy C-Means/#fuzzy-c-means","text":"\u200b Fuzzy C-Means (FCM) adalah suatu teknik pengelompokan data yang keberadaan tiap-tiap data dalam suatu kelompok ditentukan oleh nilai atau derajat keanggotaan tertentu dan teknik ini pertama kali diperkenalkan oleh Jim Bezdek pada tahun 1981. Fuzzy C-Means menerapkan pengelompokan fuzzy, dimana setiap data dapat menjadi anggota dari beberapa cluster dengan derajat keanggotaan yang berbeda-beda pada setiap cluster. \u200b Fuzzy C-Means merupakan algoritma iteratif, yang menerapkan iterasi pada proses clustering data. Tujuan dari Fuzzy C-Means adalah untuk mendapatkan pusat cluster yang nantinya akan digunakan untuk mengetahui data yang masuk ke dalam sebuah cluster. Dalam teori fuzzy, keanggotaan sebuah data tidak diberikan nilai secara tegas dengan nilai 1(menjadi anggota) dan nilai 0 (tidak menjadi anggota), melaikan dengan suatu nilai derajat keanggotaannya yang jangkauan nilainya 0 sampai 1. Nilai keanggotaan suatu data dalam sebuah himpunan menjadi 0 ketika sama sekali tidak menjadi anggota dan menjadi 1 ketika menjadi anggota secara penuh dalam suatu himpunan.Umumnya nilai keanggotaannya antara 0 sampai 1. semakin tinggi nilai keanggotaanya semakin tinggi derajat keanggotaanya dan semakin kecil maka semakin rendah derajat keanggotaanya. Kaitannya dengan K-means sebenarnya FCM merupakan versi fuzzy dan k-meansdengan beberapa modifikasi yang membedakan dengen K-Means. \u200b Konsep dari Fuzzy C-Means pertama kali adalah menentukan pusat cluster, yang akan menandai lokasi rata-rata untuk tiap-tiap cluster. Pada kondisi awal, pusat cluster ini masih belum akurat. Tiap-tiap titik data memiliki derajat keanggotaan untuk tiap-tiap cluster. Dengan cara memperbaiki pusat cluster dan derajat keanggotaan tiap-tiap titik data secara berulang, maka akan dapat dilihat bahwa pusat cluster akan bergerak menuju lokasi yang tepat. Perulangan ini didasarkan pada minimasi fungsi obyektif yang menggambarkan jarak dari titik data yang diberikan kepusat cluster yang terbobot oleh derajat keanggotaan titik data tersebut. Output dari Fuzzy C-Means merupakan deretan usat cluster dan beberapa derajat keanggotaan untuk tiap-tiap titik data. Informasi ini dapat digunakan untuk membangun suatu fuzzy inference system. Kelebihan dari metode fuzzy C-means adalah sederhana, mudah diimplementasikan, memiliki kemampuan untuk mengelompokkan data yang besar, dan Running timenya linear O( linear O(NCT).","title":"Fuzzy C-Means"},{"location":"Tugas 6 - Fuzzy C-Means/#algoritma-fuzzy-c-means","text":"from fcmeans import FCM from sklearn.datasets import make_blobs from matplotlib import pyplot as plt from seaborn import scatterplot as scatter # create artifitial dataset n_samples = 50000 n_bins = 3 # use 3 bins for calibration_curve as we have 3 clusters here centers = [(-5, -5), (0, 0), (5, 5)] X,_ = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0, centers=centers, shuffle=False, random_state=42) # fit the fuzzy-c-means fcm = FCM(n_clusters=3) fcm.fit(X) # outputs fcm_centers = fcm.centers fcm_labels = fcm.u.argmax(axis=1) # plot result %matplotlib inline f, axes = plt.subplots(1, 2, figsize=(11,5)) scatter(X[:,0], X[:,1], ax=axes[0]) scatter(X[:,0], X[:,1], ax=axes[1], hue=fcm_labels) scatter(fcm_centers[:,0], fcm_centers[:,1], ax=axes[1],marker=\"s\",s=200) plt.show() MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Algoritma fuzzy c-means"}]}