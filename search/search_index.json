{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Biodata Nama : Ella Dwi Novitasari NIM : 180411100126 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab,S.SI.,M.kom Jurusan : Teknik Informatika Alamat : Perumahan Graha Indah N-12A, Lamongan","title":"Biodata"},{"location":"#biodata","text":"Nama : Ella Dwi Novitasari NIM : 180411100126 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab,S.SI.,M.kom Jurusan : Teknik Informatika Alamat : Perumahan Graha Indah N-12A, Lamongan","title":"Biodata"},{"location":"Statistik Deskriptif/","text":"Pengertian Statistik Deskriptif adalah metode \u2013 metode yang juga berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga dapat memberikan informasi yang berguna. Statistik Deskriptif juga merupakan metode yang sangat sederhana. Metode ini hanya mendeskripsikan kondisi dari data yang sudah anda miliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang disajikan dalam uraian \u2013 uraian singkat dan juga terbatas. Statistik deskriptif hanya terbatas dalam menyajikan data dalam bentuk tabel, diagram, grafik, dan besaran lain, sedangkan statistik inferensial selain mencakup statistik deskriptif juga dapat digunakan untuk melakukan estimasi dan penarikan kesimpulan terhadap populasi dari sampelnya. Untuk sampai pada penarikan kesimpulan statistik inferensia melalui tahap uji hipotesis dan uji statistik. Tipe Data Statistik Deskriptif Mean ( Rata-rata ) Mean merupakan teknik penjelasan kelompok yang didasarkan atas nilai rata-rata dari kelompok tersebut. Mean ini didapat dengan menjumlahkan data seluruh individu dalam kelompok itu, kemudian dibagi dengan jumlah individu yang ada pada kelompok tersebut. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Diketahui : x bar = x rata - rata = nilai rata - rata sampel x = data ke n n = banyaknya data Median ( Nilai Tengah ) Median merupakan salah satu teknik penjelasan kelompok yang didasarkan atas nilai tengah dari kelompok data yang telah disusun urutannya dari yang terkecil sampai yang terbesar, atau sebaliknya dari yang terbesar sampai yang terkecil. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ Diketahui : Me = media dari kelompok data n = banyak data Modus Modus merupakan teknik penjelasan kelompok yang didasarkan atas nilai yang sedang populer (yang sedang menjadi mode) atau nilai yang sering muncul dalam kelompok tersebut. Terdapat dua macam modus untuk sebuah data yaitu modus untuk data tunggal dan modus untuk data golongan. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Diketahui : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemen sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen selanjutnya p = panjang interval nilai b1 dan b2 selalu mutlak(bernilai positif) Varian dan Standar Deviasi Standar deviasi dan varian salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varian merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar deviasi dan varians simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Diketahui : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari anggota data Skewness Skewness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan miring atau tidaknya bentuk kurva suatu distribusi frekuensi. Skewness adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Diketahui : Xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi Quartile Quartile adalah nilai-nilai yang membagi data dalam 4 bagian yang sama, atau setiap bagian dari kuartil sebesar 25%. Quartile itu ada 3, yaitu quartile pertama, kedua, dan ketiga. Adapun cara mencarinya hampir sama dengan cara mencari median, perbedaanya pada letak quartile. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Diketahui : Q = nilai dari quartile n = banyak dari himpunan data Penerapan Statistik Deskriptif import pandas as pd from scipy import stats df = pd.read_csv('Tugas 1_Penambangan Data.csv',sep=';') data = {\"stats\": ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes','Quantile 1','Quantile 2', 'Quantile 3','Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(),2), round(df[i].var(),2), round(df[i].skew(),2), df[i].quantile(0.25),df[i].quantile(0.5),df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]] tes = pd.DataFrame(data) tes.style.hide_index() Hasil run : stats Tinggi Badan Berat Badan Usia Lingkar Badan Min 140 40 20 70 Max 190 70 50 100 Mean 164.882 54.72 34.832 85.228 Standard Deviasi 15.18 8.96 9.3 8.8 Variasi 230.35 80.27 86.4 77.42 Skewnes -0 0.1 0.08 -0.07 Quantile 1 151 47 27 78 Quantile 2 165 54 34 85 Quantile 3 179 63 43.25 93 Median 165 54 34 85 Modus 142 50 28 93 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Statistik Deskriptif"},{"location":"Statistik Deskriptif/#pengertian","text":"Statistik Deskriptif adalah metode \u2013 metode yang juga berkaitan dengan pengumpulan dan penyajian suatu gugus data sehingga dapat memberikan informasi yang berguna. Statistik Deskriptif juga merupakan metode yang sangat sederhana. Metode ini hanya mendeskripsikan kondisi dari data yang sudah anda miliki dan menyajikannya dalam bentuk tabel diagram grafik dan bentuk lainnya yang disajikan dalam uraian \u2013 uraian singkat dan juga terbatas. Statistik deskriptif hanya terbatas dalam menyajikan data dalam bentuk tabel, diagram, grafik, dan besaran lain, sedangkan statistik inferensial selain mencakup statistik deskriptif juga dapat digunakan untuk melakukan estimasi dan penarikan kesimpulan terhadap populasi dari sampelnya. Untuk sampai pada penarikan kesimpulan statistik inferensia melalui tahap uji hipotesis dan uji statistik.","title":"Pengertian"},{"location":"Statistik Deskriptif/#tipe-data-statistik-deskriptif","text":"","title":"Tipe Data Statistik Deskriptif"},{"location":"Statistik Deskriptif/#mean-rata-rata","text":"Mean merupakan teknik penjelasan kelompok yang didasarkan atas nilai rata-rata dari kelompok tersebut. Mean ini didapat dengan menjumlahkan data seluruh individu dalam kelompok itu, kemudian dibagi dengan jumlah individu yang ada pada kelompok tersebut. $$ \\bar x ={\\sum \\limits_{i=1}^{n} x_i \\over N} = {x_1 + x_2 + x_3 + ... + x_n \\over N} $$ Diketahui : x bar = x rata - rata = nilai rata - rata sampel x = data ke n n = banyaknya data","title":"Mean ( Rata-rata )"},{"location":"Statistik Deskriptif/#median-nilai-tengah","text":"Median merupakan salah satu teknik penjelasan kelompok yang didasarkan atas nilai tengah dari kelompok data yang telah disusun urutannya dari yang terkecil sampai yang terbesar, atau sebaliknya dari yang terbesar sampai yang terkecil. $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ Diketahui : Me = media dari kelompok data n = banyak data","title":"Median ( Nilai Tengah )"},{"location":"Statistik Deskriptif/#modus","text":"Modus merupakan teknik penjelasan kelompok yang didasarkan atas nilai yang sedang populer (yang sedang menjadi mode) atau nilai yang sering muncul dalam kelompok tersebut. Terdapat dua macam modus untuk sebuah data yaitu modus untuk data tunggal dan modus untuk data golongan. $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ Diketahui : Mo = modus dari kelompok data Tb = tepi bawah dari elemen modus b1 = selisih frekuensi antara elemen modus dengan elemen sebelumnya b2 = selisih frekuensi antara elemen modus dengan elemen selanjutnya p = panjang interval nilai b1 dan b2 selalu mutlak(bernilai positif)","title":"Modus"},{"location":"Statistik Deskriptif/#varian-dan-standar-deviasi","text":"Standar deviasi dan varian salah satu teknik statistik yg digunakan untuk menjelaskan homogenitas kelompok. Varian merupakan jumlah kuadrat semua deviasi nilai-nilai individual terhadap rata-rata kelompok. Sedangkan akar dari varians disebut dengan standar deviasi atau simpangan baku. Standar deviasi dan varians simpangan baku merupakan variasi sebaran data. Semakin kecil nilai sebarannya berarti variasi nilai data makin sama Jika sebarannya bernilai 0, maka nilai semua datanya adalah sama. Semakin besar nilai sebarannya berarti data semakin bervariasi. $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Diketahui : Xi = titik data x bar = rata-rata dari semua titik data n = banyak dari anggota data","title":"Varian dan Standar Deviasi"},{"location":"Statistik Deskriptif/#skewness","text":"Skewness atau disebut juga ukuran kemiringan yaitu suatu bilangan yang dapat menunjukan miring atau tidaknya bentuk kurva suatu distribusi frekuensi. Skewness adalah derajat ketidaksimetrisan suatu distribusi. Jika kurva frekuensi suatu distribusi memiliki ekor yang lebih memanjang ke kanan (dilihat dari meannya) maka dikatakan menceng kanan (positif) dan jika sebaliknya maka menceng kiri (negatif). Secara perhitungan, skewness adalah momen ketiga terhadap mean. Distribusi normal (dan distribusi simetris lainnya, misalnya distribusi t atau Cauchy) memiliki skewness 0 (nol). $$ Skewness = {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Diketahui : Xi = titik data x bar = rata-rata dari distribusi n = jumlah titik dalam distribusi o = standar deviasi","title":"Skewness"},{"location":"Statistik Deskriptif/#quartile","text":"Quartile adalah nilai-nilai yang membagi data dalam 4 bagian yang sama, atau setiap bagian dari kuartil sebesar 25%. Quartile itu ada 3, yaitu quartile pertama, kedua, dan ketiga. Adapun cara mencarinya hampir sama dengan cara mencari median, perbedaanya pada letak quartile. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Diketahui : Q = nilai dari quartile n = banyak dari himpunan data","title":"Quartile"},{"location":"Statistik Deskriptif/#penerapan-statistik-deskriptif","text":"import pandas as pd from scipy import stats df = pd.read_csv('Tugas 1_Penambangan Data.csv',sep=';') data = {\"stats\": ['Min','Max','Mean','Standard Deviasi','Variasi','Skewnes','Quantile 1','Quantile 2', 'Quantile 3','Median','Modus']} for i in df.columns: data[i] = [df[i].min(), df[i].max(), df[i].mean(), round(df[i].std(),2), round(df[i].var(),2), round(df[i].skew(),2), df[i].quantile(0.25),df[i].quantile(0.5),df[i].quantile(0.75), df[i].median(), stats.mode(df[i]).mode[0]] tes = pd.DataFrame(data) tes.style.hide_index() Hasil run : stats Tinggi Badan Berat Badan Usia Lingkar Badan Min 140 40 20 70 Max 190 70 50 100 Mean 164.882 54.72 34.832 85.228 Standard Deviasi 15.18 8.96 9.3 8.8 Variasi 230.35 80.27 86.4 77.42 Skewnes -0 0.1 0.08 -0.07 Quantile 1 151 47 27 78 Quantile 2 165 54 34 85 Quantile 3 179 63 43.25 93 Median 165 54 34 85 Modus 142 50 28 93 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Penerapan Statistik Deskriptif"},{"location":"Tugas 2 - Mengukur Jarak/","text":"MENGUKUR JARAK Pengukuran jarak memegang peran yang sangat penting dalam menentukan kemiripan atau keteraturan di antara data dan item. Hal ini dilakukan untuk mengetahui, dengan cara seperti apa data dikatakan saling terkait, mirip, tidak mirip, dan metode pengukuran jarak seperti apa yang diperlukan untuk membandingkannya. Pada proses clustering, tahapan menentukan atau mendeskripsikan nilai kuantitatif dari tingkat kemiripan atau ketidakmiripan data (proximity measure) memiliki peranan sangat penting, sehingga perlu dilakukannya perbandingan beberapa metode yang sering digunakan, yaitu jarak euclidean, manhattan, dan minkowski. Mengukur jarak menggunakan Eucliean Distance Euclidean distance merupakan salah satu metode perhitungan jarak yang digunakan untuk mengukur jarak dari 2(dua) buah titik dalam Euclidean space (meliputi bidang euclidean dua dimensi, tiga dimensi, atau bahkan lebih). Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini. Mengukur jarak menggunakan Manhattan Distance Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Manhattan distance digunakan untuk menghitung perbedaan absolut (mutlak) antara koordinat sepasang objek. Rumus yang digunakan adalah sebagai berikut: $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ dimana m merupakan bilangan riel positif xi dan yi merupakan dua vektor dalam ruang dimensi n implementasi ukuran jarak minkowski pada model clustering data atribut dilakukan normalisasi untuk mengukur dominasi dari atribut yang memiliki skala data besar,. Mengukur jarak menggunakan Minkowski Distance Minkowski distance merupakan sebuah metrik dalam ruang vektor di mana suatu norma didefinisikan (normed vector space) sekaligus dianggap sebagai generalisasi dari Euclidean distance dan Manhattan distance. Dalam pengukuran jarak objek menggunakan minkowski distance biasanya digunakan nilai p adalah 1 atau 2. Berikut rumus yang digunakan menghitung jarak dalam metode ini. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); Langkah mengukur jarak : #Tugas 2_Penambangan Data.csv import pandas as pd from scipy import stats df = pd.read_csv('Tugas 2_Penambangan Data.csv',nrows=4,sep=';') df import scipy.spatial.distance as minko import itertools def minkowski (x,y,data): return sum(x)+sum(y) dfvalues = df.values.tolist() data = [ [x[0],x[1],minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 1) ,minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 2)] for x in itertools.combinations(range(4),2) ] columns = ['x','y', 'Minkowski (m-1)', 'Minkowski (m-2)'] pd.DataFrame(data, columns=columns) numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"x\"]+['y']+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [0]+[2]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [0]+[3]+[0]+[\"{:.2f}\".format(chordDist(0,3,numerical))]+[0]+[0]+[0], [1]+[2]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [1]+[3]+[0]+[\"{:.2f}\".format(chordDist(1,3,numerical))]+[0]+[0]+[0], [2]+[3]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html')))","title":"MENGUKUR JARAK"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak","text":"Pengukuran jarak memegang peran yang sangat penting dalam menentukan kemiripan atau keteraturan di antara data dan item. Hal ini dilakukan untuk mengetahui, dengan cara seperti apa data dikatakan saling terkait, mirip, tidak mirip, dan metode pengukuran jarak seperti apa yang diperlukan untuk membandingkannya. Pada proses clustering, tahapan menentukan atau mendeskripsikan nilai kuantitatif dari tingkat kemiripan atau ketidakmiripan data (proximity measure) memiliki peranan sangat penting, sehingga perlu dilakukannya perbandingan beberapa metode yang sering digunakan, yaitu jarak euclidean, manhattan, dan minkowski.","title":"MENGUKUR JARAK"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak-menggunakan-eucliean-distance","text":"Euclidean distance merupakan salah satu metode perhitungan jarak yang digunakan untuk mengukur jarak dari 2(dua) buah titik dalam Euclidean space (meliputi bidang euclidean dua dimensi, tiga dimensi, atau bahkan lebih). Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini.","title":"Mengukur jarak menggunakan Eucliean Distance"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak-menggunakan-manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Manhattan distance digunakan untuk menghitung perbedaan absolut (mutlak) antara koordinat sepasang objek. Rumus yang digunakan adalah sebagai berikut: $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ dimana m merupakan bilangan riel positif xi dan yi merupakan dua vektor dalam ruang dimensi n implementasi ukuran jarak minkowski pada model clustering data atribut dilakukan normalisasi untuk mengukur dominasi dari atribut yang memiliki skala data besar,.","title":"Mengukur jarak menggunakan Manhattan Distance"},{"location":"Tugas 2 - Mengukur Jarak/#mengukur-jarak-menggunakan-minkowski-distance","text":"Minkowski distance merupakan sebuah metrik dalam ruang vektor di mana suatu norma didefinisikan (normed vector space) sekaligus dianggap sebagai generalisasi dari Euclidean distance dan Manhattan distance. Dalam pengukuran jarak objek menggunakan minkowski distance biasanya digunakan nilai p adalah 1 atau 2. Berikut rumus yang digunakan menghitung jarak dalam metode ini. $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Mengukur jarak menggunakan Minkowski Distance"},{"location":"Tugas 2 - Mengukur Jarak/#langkah-mengukur-jarak","text":"#Tugas 2_Penambangan Data.csv import pandas as pd from scipy import stats df = pd.read_csv('Tugas 2_Penambangan Data.csv',nrows=4,sep=';') df import scipy.spatial.distance as minko import itertools def minkowski (x,y,data): return sum(x)+sum(y) dfvalues = df.values.tolist() data = [ [x[0],x[1],minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 1) ,minko.minkowski(dfvalues[x[0]], dfvalues[x[1]], 2)] for x in itertools.combinations(range(4),2) ] columns = ['x','y', 'Minkowski (m-1)', 'Minkowski (m-2)'] pd.DataFrame(data, columns=columns) numerical=[0,3] categorical=[1,2,6,7] binary=[4,5,8] ordinal=[1,2] def chordDist(v1,v2,jnis): jmlh=0 normv1=0 normv2=0 for x in range (len(jnis)): normv1=normv1+(int(df.values.tolist()[v1][jnis[x]])**2) normv2=normv2+(int(df.values.tolist()[v1][jnis[x]])**2) jmlh=jmlh+(int(df.values.tolist()[v1][jnis[x]])*int(df.values.tolist()[v2][jnis[x]])) return ((2-(2*jmlh/(normv1*normv2)))**0.5) from IPython.display import HTML, display import tabulate table=[ [\"x\"]+['y']+[\"Jarak\"]+[\"Numeric\"]+[\"Ordinal\"]+[\"Categorical\"]+[\"Binary\"], [0]+[2]+[0]+[\"{:.2f}\".format(chordDist(0,2,numerical))]+[0]+[0]+[0], [0]+[3]+[0]+[\"{:.2f}\".format(chordDist(0,3,numerical))]+[0]+[0]+[0], [1]+[2]+[0]+[\"{:.2f}\".format(chordDist(1,2,numerical))]+[0]+[0]+[0], [1]+[3]+[0]+[\"{:.2f}\".format(chordDist(1,3,numerical))]+[0]+[0]+[0], [2]+[3]+[0]+[\"{:.2f}\".format(chordDist(2,3,numerical))]+[0]+[0]+[0], ] display(HTML(tabulate.tabulate(table, tablefmt='html')))","title":"Langkah mengukur jarak :"},{"location":"Tugas 3 - Missing Value with KNN/","text":"MISSING VALUE WITH KNN Missing Data adalah suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. Salah satu metode imputasi adalah metode K Nearest Neighbor(KNN). Penelitian ini dilakukan untuk memprediksi nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K adalah jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15. Imputasi dengan Metode KNN Salah satu metode yang sering digunakan untuk masalah imputasi missing data adalah KNN. Metode ini merupakan metode yang sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc Kelemahan dan Keuntungan KNN Keuntungan KNN adalah karena metode ini dapat digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. Selain itu Algoritma KNN akan mencari data yang paling mirip pada seluruh data pada dataset. Hal ini mengakibatkan kompleksitas waktu yang dibutuhkan algoritma ini menjadi cukup tinggi. Kelemahan yang ketiga adalah Pemilihan nilai k yang tidak tepat dapat menurukan kinerja klasifikasi (Acuna, 2004). # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = {'First Score':[100, 90, np.nan, 95], 'Second Score': [30, 45, 56, np.nan], 'Third Score':[np.nan, 40, 80, 98]} # creating a dataframe from dictionary df = pd.DataFrame(dict) # filling missing value using fillna() df.fillna(0)","title":"MISSING VALUE WITH KNN"},{"location":"Tugas 3 - Missing Value with KNN/#missing-value-with-knn","text":"Missing Data adalah suatu informasi yang tidak tersedia dalam suatu data. Missing data mempengaruhi hasil penelitian karena keberadaan missing data dapat mengurangi tingkat akurasi dari hasil penelitian. Missing data dapat diatasi dengan imputasi. Imputasi merupakan suatu metode yang mengatasi missing data dengan mengisi nilai yang hilang dengan suatu nilai berdasarkan informasi lain yang didapat dari data tersebut. Salah satu metode imputasi adalah metode K Nearest Neighbor(KNN). Penelitian ini dilakukan untuk memprediksi nilai yang hilang dengan metode KNN. KNN bekerja dengan menghitung weight mean estimation berdasarkan jumlah K. K adalah jumlah observasi terdekat yang akan digunakan. Dalam penelitian ini, K yang digunakan yaitu K = 1, K = 5, K = 10, K = 15, dan K=20. Mean Square Error (MSE) dan Mean Absolute Percentage Error (MAPE) digunakan untuk mengetahui ketepatan hasil imputasi. Berdasarkan nilai rata-rata MSE dan MAPE dari 10 replikasi, KNN terbaik pada missing data 10% dan 20% terjadi pada saat K = 10, sedangkan untuk missing data 30% terjadi saat K = 15.","title":"MISSING VALUE WITH KNN"},{"location":"Tugas 3 - Missing Value with KNN/#imputasi-dengan-metode-knn","text":"Salah satu metode yang sering digunakan untuk masalah imputasi missing data adalah KNN. Metode ini merupakan metode yang sederhana dan fleksibel karena dapat digunakan baik pada variabel dengan data kontinu maupun data diskrit (Mawarsari, 2012). Algoritma KNN : Tentukan nilai k Tentukan jarak Euclidian antar instance pada dataset Dm dan dataset Dc Imputasi data hilang dengan rata-rata k tetangga terdekat di Dc","title":"Imputasi dengan Metode KNN"},{"location":"Tugas 3 - Missing Value with KNN/#kelemahan-dan-keuntungan-knn","text":"Keuntungan KNN adalah karena metode ini dapat digunakan untuk data yang bersifat kualitatif maupun kuantitatif, tanpa perlu membuat model prediksi, algoritma sederhana, KNN dibutuhkan untuk pertimbangan struktur korelasi data (Acuna, 2004). Sedangkan kelemahan KNN adalah adanya pemilihan fungsi jarak, dapat menggunakan Euclidean, Manhattan, Mahalanobis dan Pearson. Selain itu Algoritma KNN akan mencari data yang paling mirip pada seluruh data pada dataset. Hal ini mengakibatkan kompleksitas waktu yang dibutuhkan algoritma ini menjadi cukup tinggi. Kelemahan yang ketiga adalah Pemilihan nilai k yang tidak tepat dapat menurukan kinerja klasifikasi (Acuna, 2004). # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = {'First Score':[100, 90, np.nan, 95], 'Second Score': [30, 45, 56, np.nan], 'Third Score':[np.nan, 40, 80, 98]} # creating a dataframe from dictionary df = pd.DataFrame(dict) # filling missing value using fillna() df.fillna(0)","title":"Kelemahan dan Keuntungan KNN"},{"location":"Tugas 4 - Decision Tree/","text":"DECISION TREE Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Entropy Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row Gain Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur Gini Index Dalam penerapan GINI index untuk data berskala continuous , terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode brute-force dan metode midpoints . # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata(): balance_data = pd.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-'+ 'databases/balance-scale/balance-scale.data', sep= ',', header = None) # Printing the dataswet shape print (\"Dataset Length: \", len(balance_data)) print (\"Dataset Shape: \", balance_data.shape) # Printing the dataset obseravtions print (\"Dataset: \",balance_data.head()) return balance_data # Function to split the dataset def splitdataset(balance_data): # Seperating the target variable X = balance_data.values[:, 1:5] Y = balance_data.values[:, 0] # Spliting the dataset into train and test X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100) return X, Y, X_train, X_test, y_train, y_test # Function to perform training with giniIndex. def train_using_gini(X_train, X_test, y_train): # Creating the classifier object clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5) # Performing training clf_gini.fit(X_train, y_train) return clf_gini # Function to perform training with entropy. def tarin_using_entropy(X_train, X_test, y_train): # Decision tree with entropy clf_entropy = DecisionTreeClassifier( criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5) # Performing training clf_entropy.fit(X_train, y_train) return clf_entropy # Function to make predictions def prediction(X_test, clf_object): # Predicton on test with giniIndex y_pred = clf_object.predict(X_test) print(\"Predicted values:\") print(y_pred) return y_pred # Function to calculate accuracy def cal_accuracy(y_test, y_pred): print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred)) print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) print(\"Report : \", classification_report(y_test, y_pred)) # Driver code def main(): # Building Phase data = importdata() X, Y, X_train, X_test, y_train, y_test = splitdataset(data) clf_gini = train_using_gini(X_train, X_test, y_train) clf_entropy = tarin_using_entropy(X_train, X_test, y_train) # Operational Phase print(\"Results Using Gini Index:\") # Prediction using gini y_pred_gini = prediction(X_test, clf_gini) cal_accuracy(y_test, y_pred_gini) print(\"Results Using Entropy:\") # Prediction using entropy y_pred_entropy = prediction(X_test, clf_entropy) cal_accuracy(y_test, y_pred_entropy) # Calling main function if __name__==\"__main__\": main() Dataset Length: 625 Dataset Shape: (625, 5) Dataset: 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index: Predicted values: ['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 67 18] [ 0 19 71]] Accuracy : 73.40425531914893 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy: Predicted values: ['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 63 22] [ 0 20 70]] Accuracy : 70.74468085106383 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"**DECISION TREE**"},{"location":"Tugas 4 - Decision Tree/#decision-tree","text":"Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain.","title":"DECISION TREE"},{"location":"Tugas 4 - Decision Tree/#entropy","text":"Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row","title":"Entropy"},{"location":"Tugas 4 - Decision Tree/#gain","text":"Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur","title":"Gain"},{"location":"Tugas 4 - Decision Tree/#gini-index","text":"Dalam penerapan GINI index untuk data berskala continuous , terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode brute-force dan metode midpoints . # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata(): balance_data = pd.read_csv( 'https://archive.ics.uci.edu/ml/machine-learning-'+ 'databases/balance-scale/balance-scale.data', sep= ',', header = None) # Printing the dataswet shape print (\"Dataset Length: \", len(balance_data)) print (\"Dataset Shape: \", balance_data.shape) # Printing the dataset obseravtions print (\"Dataset: \",balance_data.head()) return balance_data # Function to split the dataset def splitdataset(balance_data): # Seperating the target variable X = balance_data.values[:, 1:5] Y = balance_data.values[:, 0] # Spliting the dataset into train and test X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100) return X, Y, X_train, X_test, y_train, y_test # Function to perform training with giniIndex. def train_using_gini(X_train, X_test, y_train): # Creating the classifier object clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,max_depth=3, min_samples_leaf=5) # Performing training clf_gini.fit(X_train, y_train) return clf_gini # Function to perform training with entropy. def tarin_using_entropy(X_train, X_test, y_train): # Decision tree with entropy clf_entropy = DecisionTreeClassifier( criterion = \"entropy\", random_state = 100, max_depth = 3, min_samples_leaf = 5) # Performing training clf_entropy.fit(X_train, y_train) return clf_entropy # Function to make predictions def prediction(X_test, clf_object): # Predicton on test with giniIndex y_pred = clf_object.predict(X_test) print(\"Predicted values:\") print(y_pred) return y_pred # Function to calculate accuracy def cal_accuracy(y_test, y_pred): print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred)) print (\"Accuracy : \", accuracy_score(y_test,y_pred)*100) print(\"Report : \", classification_report(y_test, y_pred)) # Driver code def main(): # Building Phase data = importdata() X, Y, X_train, X_test, y_train, y_test = splitdataset(data) clf_gini = train_using_gini(X_train, X_test, y_train) clf_entropy = tarin_using_entropy(X_train, X_test, y_train) # Operational Phase print(\"Results Using Gini Index:\") # Prediction using gini y_pred_gini = prediction(X_test, clf_gini) cal_accuracy(y_test, y_pred_gini) print(\"Results Using Entropy:\") # Prediction using entropy y_pred_entropy = prediction(X_test, clf_entropy) cal_accuracy(y_test, y_pred_entropy) # Calling main function if __name__==\"__main__\": main() Dataset Length: 625 Dataset Shape: (625, 5) Dataset: 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index: Predicted values: ['R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 67 18] [ 0 19 71]] Accuracy : 73.40425531914893 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy: Predicted values: ['R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R'] Confusion Matrix: [[ 0 6 7] [ 0 63 22] [ 0 20 70]] Accuracy : 70.74468085106383 Report : precision recall f1-score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} });","title":"Gini Index"}]}